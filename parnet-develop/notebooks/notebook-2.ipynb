{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNLLLossFromLogits(nn.Module):\n",
    "    def __init__(self, reduction=torch.mean):\n",
    "        super(MultinomialNLLLossFromLogits, self).__init__()\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def __call__(self, y, y_pred):\n",
    "        return self.log_likelihood_from_logits(y, y_pred)\n",
    "\n",
    "    def log_likelihood_from_logits(self, y, y_pred):\n",
    "        log_prob = -torch.sum(torch.mul(torch.log_softmax(y_pred, dim=-1), y), dim=-1) * self.log_combinations(y)\n",
    "        if self.reduction is not None:\n",
    "            return self.reduction(log_prob)\n",
    "        return log_prob\n",
    "\n",
    "    def log_combinations(self, input):\n",
    "        total_permutations = torch.lgamma(torch.sum(input, dim=-1) + 1)\n",
    "        counts_factorial = torch.lgamma(input + 1)\n",
    "        redundant_permutations = torch.sum(counts_factorial, dim=-1)\n",
    "        return total_permutations - redundant_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DFirstLayer(nn.Module):\n",
    "    def __init__(self, in_chan, filters=128, kernel_size=12):\n",
    "        super(Conv1DFirstLayer, self).__init__()\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_chan, filters, kernel_size=kernel_size, padding='same')\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        x = self.conv1d(inputs)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DResBlock(nn.Module):\n",
    "    def __init__(self, in_chan, filters=128, kernel_size=3, dropout=0.25, dilation=1, residual=True):\n",
    "        super(Conv1DResBlock, self).__init__()\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_chan, filters, kernel_size=kernel_size, dilation=dilation, padding='same')\n",
    "        self.batch_norm = nn.BatchNorm1d(filters)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.residual = residual\n",
    "    \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        x = self.conv1d(inputs)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.residual:\n",
    "            x = inputs + x\n",
    "        return x\n",
    "\n",
    "# %%\n",
    "class IndexEmbeddingOutputHead(nn.Module):\n",
    "    def __init__(self, n_tasks, dims):\n",
    "        super(IndexEmbeddingOutputHead, self).__init__()\n",
    "\n",
    "        # protein/experiment embedding of shape (p, d)\n",
    "        self.embedding = torch.nn.Embedding(n_tasks, dims)\n",
    "    \n",
    "    def forward(self, bottleneck, **kwargs):\n",
    "        # bottleneck of shape (batch, d, n) --> (batch, n, d)\n",
    "        bottleneck = torch.transpose(bottleneck, -1, -2)\n",
    "        \n",
    "        # embedding of (batch, p, d) --> (batch, d, p)\n",
    "        embedding = torch.transpose(self.embedding.weight, 0, 1)\n",
    "\n",
    "        logits = torch.matmul(bottleneck, embedding) # torch.transpose(self.embedding.weight, 0, 1)  \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexEmbeddingOutputHead(nn.Module):\n",
    "    def __init__(self, n_tasks, dims):\n",
    "        super(IndexEmbeddingOutputHead, self).__init__()\n",
    "\n",
    "        # protein/experiment embedding of shape (p, d)\n",
    "        self.embedding = torch.nn.Embedding(n_tasks, dims)\n",
    "    \n",
    "    def forward(self, bottleneck, **kwargs):\n",
    "        # bottleneck of shape (batch, d, n) --> (batch, n, d)\n",
    "        bottleneck = torch.transpose(bottleneck, -1, -2)\n",
    "        \n",
    "        # embedding of (batch, p, d) --> (batch, d, p)\n",
    "        embedding = torch.transpose(self.embedding.weight, 0, 1)\n",
    "\n",
    "        logits = torch.matmul(bottleneck, embedding) # torch.transpose(self.embedding.weight, 0, 1)  \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, tasks, nlayers=9):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.tasks = tasks\n",
    "\n",
    "        self.body = nn.Sequential(*[Conv1DFirstLayer(4, 128)]+[(Conv1DResBlock(128, dilation=(2**i))) for i in range(nlayers)])\n",
    "        self.head = IndexEmbeddingOutputHead(len(self.tasks), dims=128)\n",
    "    \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "\n",
    "        for layer in self.body:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(tasks=list(range(223)))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net(torch.rand(2, 4, 201))\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioflow import io\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "def load_tf_dataset_to_torch(filepath, features_filepath=None, batch_size=64, cache=True, shuffle=None):\n",
    "    dataset = io.dataset_ops.load_tfrecord(filepath, deserialize=False)\n",
    "\n",
    "    # cache\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle)\n",
    "\n",
    "    # deserialize\n",
    "    if features_filepath is None:\n",
    "        features_filepath = filepath + '.features.json'\n",
    "    features = io.dataset_ops.features_from_json_file(features_filepath)\n",
    "    dataset = io.dataset_ops.deserialize_dataset(dataset, features)\n",
    "\n",
    "    # batch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # format dataset\n",
    "    dataset = dataset.map(lambda e: (tf.transpose(e['inputs']['input'], perm=[0, 2, 1]), e['outputs']))\n",
    "\n",
    "    for example in dataset.as_numpy_iterator():\n",
    "        # yield example\n",
    "        yield tf.nest.map_structure(lambda x: torch.tensor(x).to(torch.float32), example)\n",
    "\n",
    "torch_dataset = load_tf_dataset_to_torch('example-data-matrix/windows.chr13.4.data.matrix.filtered.tfrecord', shuffle=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, filepath, features_filepath=None, batch_size=64, cache=True, shuffle=None):\n",
    "        super(TFIterableDataset).__init__()\n",
    "\n",
    "        self.dataset = io.dataset_ops.load_tfrecord(filepath, deserialize=False)\n",
    "\n",
    "        # cache\n",
    "        if cache:\n",
    "            self.dataset = self.dataset.cache()\n",
    "\n",
    "        if shuffle:\n",
    "            self.dataset = self.dataset.shuffle(shuffle)\n",
    "\n",
    "        # deserialize\n",
    "        if features_filepath is None:\n",
    "            features_filepath = filepath + '.features.json'\n",
    "        self.features = io.dataset_ops.features_from_json_file(features_filepath)\n",
    "        self.dataset = io.dataset_ops.deserialize_dataset(self.dataset, self.features)\n",
    "\n",
    "        # batch\n",
    "        self.dataset = self.dataset.batch(batch_size)\n",
    "\n",
    "        # format dataset\n",
    "        self.dataset = self.dataset.map(lambda e: (tf.transpose(e['inputs']['input'], perm=[0, 2, 1]), e['outputs']))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for example in self.dataset.as_numpy_iterator():\n",
    "            yield tf.nest.map_structure(lambda x: torch.tensor(x).to(torch.float32), example)\n",
    "\n",
    "dataset = TFIterableDataset('example-data-matrix/windows.chr13.4.data.matrix.filtered.tfrecord', shuffle=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in dataloader:\n",
    "    print(len(s))\n",
    "    print(s[0].shape)\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(torch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[0].shape)\n",
    "print(batch[1]['signal']['total'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_dataset_generator(n=1000):\n",
    "    for _ in range(n):\n",
    "        yield (torch.rand(8, 4, 101, dtype=torch.float32), {'signal': {'total': torch.randint(10, (8, 101, 7)).to(torch.float32)}})\n",
    "\n",
    "next(iter(example_dataset_generator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f'Epoch: {epoch}/5')\n",
    "    for sample in tqdm(example_dataset_generator(100), total=100):\n",
    "        _ = net(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lambda: example_dataset_generator(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "test_net = Network(tasks=list(range(7)))\n",
    "test_net\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = MultinomialNLLLossFromLogits()\n",
    "\n",
    "def train(net, dataset, epochs=2):\n",
    "    for epoch in tqdm.trange(epochs):\n",
    "        epoch_running_loss = 0.0\n",
    "        print(f'Epoch {epoch}')\n",
    "        for sample in dataset():\n",
    "            x, y = sample\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            y_pred = net(x)\n",
    "            loss = criterion(y['signal']['total'], y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # add to running loss\n",
    "            epoch_running_loss += loss.item()\n",
    "        print(f'Loss {epoch_running_loss}')\n",
    "\n",
    "train(test_net, lambda: example_dataset_generator(100), epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(torch_dataset))[0].shape)\n",
    "print(next(iter(torch_dataset))[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lengths = []\n",
    "for e in io.load_tfrecord('example-data-matrix/windows.chr15.4.data.matrix.filtered.tfrecord'):\n",
    "    # total_lengths.append(int(e['outputs']['signal']['total'].shape[1]))\n",
    "    total_lengths.append(int(e['inputs']['input'].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(total_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.softmax(y_pred, dim=-2)\n",
    "print(res[0][:,0].shape)\n",
    "print(torch.sum(res[0][:,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_pred = torch.rand(2, 201, 128)\n",
    "print(ex_pred.shape)\n",
    "\n",
    "embed = torch.rand(128, 223)\n",
    "print(embed.shape)\n",
    "\n",
    "print(torch.unsqueeze(embed, dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(ex_pred, embed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(torch.rand(2, 3, 4), torch.rand(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0397, -0.0754, -0.0656,  0.0886])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = torchmetrics.PearsonCorrCoef(num_outputs=4)\n",
    "corr(torch.rand(101, 4), torch.rand(101, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchmetrics.functional.pearson_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = torchmetrics.PearsonCorrCoef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transparent_corr(x, y):\n",
    "    print(x.shape, y.shape)\n",
    "    # print(x[0].shape, y[0].shape)\n",
    "    # print(x)\n",
    "    # return x\n",
    "    # print(x, y)\n",
    "    # return torch.mean(x + y)\n",
    "    return torchmetrics.functional.pearson_corrcoef(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "import functorch\n",
    "\n",
    "# a, b = torch.rand(2, 101, 4), torch.rand(2, 101, 4)\n",
    "a, b = torch.rand(2, 101, 7), torch.rand(2, 101, 7)\n",
    "# print(a)\n",
    "print(torchmetrics.functional.pearson_corrcoef(a[0], b[0]).shape)\n",
    "\n",
    "\n",
    "# vmap_corr = functorch.vmap(transparent_corr, in_dims=(0, 2), out_dims=(0, 2))\n",
    "# vmap_corr = functorch.vmap(transparent_corr, in_dims=0, out_dims=0)\n",
    "# out = vmap_corr(a, b)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1186,  0.0394, -0.1787, -0.0259, -0.0223,  0.0634,  0.1822])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_pearson_corrcoef(y_batch, y_pred_batch):\n",
    "    return torch.sum(torch.stack([torchmetrics.functional.pearson_corrcoef(y_batch[i], y_pred_batch[i]) for i in range(y_batch.shape[0])]), dim=0)\n",
    "\n",
    "batched_pearson_corrcoef(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torchmetrics.functional.pearson_corrcoef(a[i], b[i]) for i in range(a.shape[0])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_pearson_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomial = torch.distributions.Multinomial(total_count=42, logits=torch.tensor([2, 3.2, 5, 1.9]))\n",
    "nll = -multinomial.log_prob(torch.tensor([7, 8, 20, 7]))\n",
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, y_pred = torch.randint(0, 10, size=(4, 42, 7)), torch.rand(4, 42, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_nll = []\n",
    "for i in range(y.shape[0]):\n",
    "    for j in range(y.shape[2]):\n",
    "        single_y, single_y_pred = y[i, :, j], y_pred[i, :, j]\n",
    "        # print(Multinomial(total_count=torch.sum(single_y), logits=single_y_pred))\n",
    "        manual_nll.append(-Multinomial(int(torch.sum(single_y)), logits=single_y_pred).log_prob(single_y))\n",
    "true_nll = torch.mean(torch.tensor(manual_nll))\n",
    "true_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNLLLossFromLogits(nn.Module):\n",
    "    def __init__(self, reduction=torch.mean):\n",
    "        super(MultinomialNLLLossFromLogits, self).__init__()\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def __call__(self, y, y_pred, dim=-1):\n",
    "        neg_log_probs = self.log_likelihood_from_logits(y, y_pred, dim) * -1\n",
    "        if self.reduction is not None:\n",
    "            return self.reduction(neg_log_probs)\n",
    "        return neg_log_probs\n",
    "\n",
    "    def log_likelihood_from_logits(self, y, y_pred, dim):\n",
    "        return torch.sum(torch.mul(torch.log_softmax(y_pred, dim=dim), y), dim=dim) + self.log_combinations(y, dim)\n",
    "\n",
    "    def log_combinations(self, input, dim):\n",
    "        total_permutations = torch.lgamma(torch.sum(input, dim=dim) + 1)\n",
    "        counts_factorial = torch.lgamma(input + 1)\n",
    "        redundant_permutations = torch.sum(counts_factorial, dim=dim)\n",
    "        return total_permutations - redundant_permutations\n",
    "\n",
    "print(y.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "nll_loss = MultinomialNLLLossFromLogits(reduction=torch.mean)\n",
    "nll = nll_loss(y, y_pred, dim=-2)\n",
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bool(true_nll == nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multinomial(total_count=int(torch.sum(single_y)), logits=single_y_pred).log_prob(single_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss(single_y, single_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(single_y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c2a629b5d736a8b2a3c0111829bdedfa4bd0b48e49067d38bd73bb54a8250f9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
